{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGu8bprtI4Wi"
   },
   "source": [
    "# Benchmarking Hub and Webdataset\n",
    "This notebook provides the methods and results of comparing two dataset management packages, Hub and WebDataset, in terms of dataset access and iteration time. This experiment expands a [blog post released by PyTorch](https://pytorch.org/blog/efficient-pytorch-io-library-for-large-datasets-many-files-many-gpus/). We use the same sample code provided in the original article and we juxtapose it with an equivalent using Hub.\n",
    "\n",
    "As in the original post, we use ImageNet Dataset. For webdataset, the data is sharded in the way as provided in the manual. The dataset was also converted into a Hub-compliant format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "\n",
    "We use AWS to run the benchmarks.\n",
    "\n",
    "Specification of the machine used for benchmarking:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Machine</th>\n",
    "    <td>AWS EC2 r5n.metal instance</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>Memory</th>\n",
    "    <td>768 GB</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>CPU</th>\n",
    "    <td>Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th>#vCPU</th>\n",
    "    <td>96</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "The data is stored locally within the instance storage.\n",
    "\n",
    "Variable parameters include:\n",
    "* number of workers (ranging from 1 to 24)\n",
    "* batch size (currently mimicking the original post: 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4enEUBQJeiI"
   },
   "source": [
    "## Installing the Dependencies\n",
    "\n",
    "First of all, we gather all the dependencies as instructed by the [tmbdev/pytorch-imagenet-wds](https://github.com/tmbdev/pytorch-imagenet-wds) repository in order to set up the environment. The hub, torch and webdataset versions are specified for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SL-dFFM_EPai"
   },
   "outputs": [],
   "source": [
    "!pip install hub==1.2.3\n",
    "!pip install torch==1.8.1\n",
    "!pip install webdataset==0.1.54\n",
    "!pip install torchvision\n",
    "!pip install braceexpand\n",
    "!pip install numpy\n",
    "!pip install scipy\n",
    "!pip install tk\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBuHzLosJ95Y"
   },
   "source": [
    "Now that the dependencies have been installed, we can focus on sharding the dataset for WebDataset. Following the instructions in the repository mentioned above, we have gotten the following script for sharding the ImageNet Dataset.\n",
    "\n",
    "There are a few parameters which can be adjusted accordingly to make the primary functions run without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ebZDnhGLCih"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "import argparse\n",
    "from torchvision import datasets\n",
    "import webdataset as wds\n",
    "import torch\n",
    "import torchvision\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Webdataset\n",
    "\n",
    "The following cell uses code from WebDataset tutorial. You need to run the following cell only once to shard the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LXV7g9xElh6"
   },
   "outputs": [],
   "source": [
    "maxsize = 1e9\n",
    "maxcount = 1000\n",
    "filekey = False\n",
    "data = \"./data\"\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.isdir(os.path.join(data, \"train\")):\n",
    "    print(f\"{data}: should be directory containing ImageNet\", file=sys.stderr)\n",
    "    print(f\"suitable as argument for torchvision.datasets.ImageNet(...)\", file=sys.stderr)\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "if not os.path.isdir(os.path.join(shards, \".\")):\n",
    "    print(f\"{shards}: should be a writable destination directory for shards\", file=sys.stderr)\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "def readfile(fname):\n",
    "    \"Read a binary file from disk.\"\n",
    "    with open(fname, \"rb\") as stream:\n",
    "        return stream.read()\n",
    "\n",
    "\n",
    "all_keys = set()\n",
    "\n",
    "\n",
    "def write_dataset(imagenet, base=\"./shards\", split=\"train\"):\n",
    "\n",
    "    # We're using the torchvision ImageNet dataset\n",
    "    # to parse the metadata; however, we will read\n",
    "    # the compressed images directly from disk (to\n",
    "    # avoid having to reencode them)\n",
    "    ds = datasets.ImageNet(imagenet, split=split)\n",
    "    nimages = len(ds.imgs)\n",
    "    print(\"# nimages\", nimages)\n",
    "\n",
    "    # We shuffle the indexes to make sure that we\n",
    "    # don't get any large sequences of a single class\n",
    "    # in the dataset.\n",
    "    indexes = list(range(nimages))\n",
    "    random.shuffle(indexes)\n",
    "\n",
    "    # This is the output pattern under which we write shards.\n",
    "    pattern = os.path.join(base, f\"imagenet-{split}-%06d.tar\")\n",
    "\n",
    "    with wds.ShardWriter(pattern, maxsize=int(maxsize), maxcount=int(maxcount)) as sink:\n",
    "        for i in indexes:\n",
    "\n",
    "            # Internal information from the ImageNet dataset\n",
    "            # instance: the file name and the numerical class.\n",
    "            fname, cls = ds.imgs[i]\n",
    "            assert cls == ds.targets[i]\n",
    "\n",
    "            # Read the JPEG-compressed image file contents.\n",
    "            image = readfile(fname)\n",
    "\n",
    "            # Construct a uniqu keye from the filename.\n",
    "            key = os.path.splitext(os.path.basename(fname))[0]\n",
    "\n",
    "            # Useful check.\n",
    "            assert key not in all_keys\n",
    "            all_keys.add(key)\n",
    "\n",
    "            # Construct a sample.\n",
    "            xkey = key if filekey else \"%07d\" % i\n",
    "            sample = {\"__key__\": xkey, \"jpg\": image, \"cls\": cls}\n",
    "\n",
    "            # Write the sample to the sharded tar archives.\n",
    "            sink.write(sample)\n",
    "\n",
    "\n",
    "# Data for WebDataset, must be in torch.Datasets.ImageNet compatible format.\n",
    "write_dataset(data)\n",
    "webdataset_url = \"/shards/imagenet-train-{000000..001281}.tar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Hub Dataset\n",
    "\n",
    "The dataset in hub format just needs to be pulled from S3 bucket to the local instance. It can also be directly streamed from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_url = \"./imagenet-hub\"\n",
    "s3_url = \"s3://internal-datasets/imagenet-classification/imagenet2012/\"\n",
    "dataset = hub.Dataset(s3_url)\n",
    "dataset.copy(hub_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWdSmBcLSiWP"
   },
   "source": [
    "# Timing the Read Access of WebDataset\n",
    "\n",
    "Now that the dataset has been sharded for WebDataset, we can start making the dataloaders to iterate over the dataset and time the read access overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the parameters with which we want to test the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKERS = [24, 16, 8, 4]\n",
    "batch_size = 1000\n",
    "\n",
    "\n",
    "def employ(workers):\n",
    "    def decorator(f):\n",
    "        def wrapper(*args):\n",
    "            times = []\n",
    "            for n in workers:\n",
    "                times.append(f(*args, n))\n",
    "            return np.round(times, 3)\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WYkTeHXFHw4a"
   },
   "outputs": [],
   "source": [
    "@employ(WORKERS)\n",
    "def time_webdataset(url, batch_size, num_workers=1):\n",
    "    dataset = wds.Dataset(url)\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    start = time.time()\n",
    "    for  name, inputs, targets in loader:\n",
    "        _, x, y =  name, inputs, targets\n",
    "    end = time.time()\n",
    "    return end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZEGD4NFXb52"
   },
   "source": [
    "# Timing the Read Access of Hub converted to PyTorch\n",
    "\n",
    "Since WebDataset is based on PyTorch and Hub offers PyTorch integration, it would be useful to compare Hub's performance when converted to PyTorch locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MChib6OpWtNn"
   },
   "outputs": [],
   "source": [
    "@employ(WORKERS)\n",
    "def time_hub(url, batch_size, num_workers=1):\n",
    "    dataset = hub.Dataset(url)\n",
    "    dataset = dataset.to_pytorch()\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=lambda b: b\n",
    "    )\n",
    "    start = time.time()\n",
    "    for batch in loader:\n",
    "        pass\n",
    "    end = time.time()\n",
    "    return end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsCuq8OjY1tG"
   },
   "source": [
    "## Running the Experiment\n",
    "\n",
    "Now that we have all the utility functions, we can run the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([232.544, 252.853, 235.366, 198.742])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_webdataset(webdataset_url, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([408.312, 375.634, 417.064, 477.035])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_hub(hub_url, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve Hub's performance, we use the remote version of Hub with a smaller batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([270.917, 254.519, 251.943, 289.542])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_hub(hub_url, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also test Hub on streaming data remotely from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1688.301, 2683.032, 4825.543, 7982.483])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_hub(s3_url, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is that Webdataset is 1.007-2.400x faster than Hub, depending on the configurations. Essentially, their performance is roughly the same, with a minor advantage of Webdataset, however given how much time is saved by avoiding any preprocessing with Hub, it is a more optimal choice for most dataset users."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "imagenet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
